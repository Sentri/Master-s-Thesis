%% using pdflatex, which directly typesets your document in
%% pdf (use jpg or pdf figures)
\documentclass[english,12pt,a4paper,pdftex,sci,utf8]{aaltothesis}
%% To the \documentclass above
%% specify your school: arts, biz, chem, elec, eng, sci
%% specify the character encoding scheme used by your editor: utf8, latin1

\usepackage{graphicx}

%% Use this if you write hard core mathematics, these are usually needed
\usepackage{amsfonts,amssymb,amsbsy}

\usepackage{tabularx}
\usepackage{csquotes}
\usepackage{biblatex}
\usepackage{enumitem}
\usepackage{textgreek}

\addbibresource{bibliography.bib}

%% Use the macros in this package to change how the hyperref package below 
%% typesets its hypertext -- hyperlink colour, font, etc. See the package
%% documentation. It also defines the \url macro, so use the package when 
%% not using the hyperref package.
%%
%\usepackage{url}

%% Use this if you want to get links and nice output. Works well with pdflatex.
\usepackage{hyperref}
\PackageWarning{NYI}{Remember to edit this}
\hypersetup{pdfpagemode=UseNone, pdfstartview=FitH,
  colorlinks=true,urlcolor=red,linkcolor=blue,citecolor=black,
  pdftitle={Default Title, Modify},pdfauthor={Teemu Vartiainen},
  pdfkeywords={Modify keywords}}

% custom command for edit comments
\definecolor{nyitext}{rgb}{0.2, 0.2, 0.2}
\definecolor{nyibg}{rgb}{1.0, 1.0, 0.6}
\newcommand{\nyi}[1]{\noindent\colorbox{nyibg}{\textcolor{nyitext}{\emph{#1}}}}

%\PackageWarning{NYI}{#1}

%% All that is printed on paper starts here
\begin{document}

%% ONLY FOR M.Sc. AND LICENTIATE THESIS: Specify your department,
%% professorship and professorship code. 
%%
\department{Department of Computer Science}
\professorship{x}
%%


%% Choose one of these:
\univdegree{MSc}

%% Your own name (should be self explanatory...)
\author{Teemu Vartiainen}

%% Your thesis title comes here and again before a possible abstract in
%% Finnish or Swedish . If the title is very long and latex does an
%% unsatisfactory job of breaking the lines, you will have to force a
%% linebreak with the \\ control character. 
%% Do not hyphenate titles.

% working title
\thesistitle{Analyzing real-time and past events for describing processes and detecting anomalies}

\place{Espoo}

%% For B.Sc. thesis use the date when you present your thesis. 
%% 
%% Kandidaatintyön päivämäärä on sen esityspäivämäärä! 
\date{x.x.2017}

%% B.Sc. or M.Sc. thesis supervisor 
%% Note the "\" after the comma. This forces the following space to be 
%% a normal interword space, not the space that starts a new sentence. 
%% This is done because the fullstop isn't the end of the sentence that
%% should be followed by a slightly longer space but is to be followed
%% by a regular space.
%%
\supervisor{Prof.\ Petri Vuorimaa}

%% B.Sc. or M.Sc. thesis advisors(s). You can give upto two advisors in
%% this template. Check with your supervisor how many official advisors
%% you can have.
%%
%\advisor{Prof.\ Pirjo Professori}
\advisor{M.Sc.\ Tao Zhu}
\advisor{M.Sc.\ Rafael Forsbach Valle}

%% Aalto logo: syntax:
%% \uselogo{aaltoRed|aaltoBlue|aaltoYellow|aaltoGray|aaltoGrayScale}{?|!|''}
%%
%% Logo language is set to be the same as the document language.
%% Logon kieli on sama kuin dokumentin kieli
%%
\uselogo{aaltoBlue}{?}

%% Create the coverpage
%%
\makecoverpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Abstracts & etc
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Note that when writting your master's thesis in English, place
%% the English abstract first followed by the possible Finnish abstract

%% English abstract.
%% All the information required in the abstract (your name, thesis title, etc.)
%% is used as specified above.
%% Specify keywords
%%
%% Kaikki tiivistelmässä tarvittava tieto (nimesi, työnnimi, jne.) käytetään
%% niin kuin se on yllä määritelty.
%% Avainsanat
%%
\keywords{event logs, machine learning, process mining}
%% Abstract text
\begin{abstractpage}[english]
  Your abstract in English. Try to keep the abstract short; approximately 
  100 words should be enough. The abstract explains your research topic, 
  the methods you have used, and the results you obtained.  
  Your abstract in English. Try to keep the abstract short; approximately 
  100 words should be enough. The abstract explains your research topic, 
  the methods you have used, and the results you obtained.  

  Your abstract in English. Try to keep the abstract short; approximately 
  100 words should be enough. The abstract explains your research topic, 
  the methods you have used, and the results you obtained.  
  Your abstract in English. Try to keep the abstract short; approximately 
  100 words should be enough. The abstract explains your research topic, 
  the methods you have used, and the results you obtained.  
\end{abstractpage}

%% Force a new page so that the possible English abstract starts on a new page
\newpage

%% Abstract in Finnish.

\thesistitle{Reaaliaikaisten ja menneiden prosessitapahtumien k\"aytt\"o prosessien kuvaamisessa}
\supervisor{Prof.\ Petri Vuorimaa}
\advisor{M.Sc.\ Tao Zhu}
\advisor{M.Sc.\ Rafael Forsbach Valle}
\department{Tietotekniikan laitos}
\professorship{x}
%% Avainsanat
\keywords{Tapahtumalokit, koneoppiminen, prosessikaaviot}
%% Tiivistelmän tekstiosa
\begin{abstractpage}[finnish]
  Tiivistelmässä on lyhyt selvitys (noin 100 sanaa)
  kirjoituksen tärkeimmästä sisällöstä: mitä ja miten on tutkittu,
  sekä mitä tuloksia on saatu. 
  Tiivistelmässä on lyhyt selvitys (noin 100 sanaa)
  kirjoituksen tärkeimmästä sisällöstä: mitä ja miten on tutkittu,
  sekä mitä tuloksia on saatu. 

  Tiivistelmässä on lyhyt selvitys (noin 100 sanaa)
  kirjoituksen tärkeimmästä sisällöstä: mitä ja miten on tutkittu,
  sekä mitä tuloksia on saatu. 
  Tiivistelmässä on lyhyt selvitys (noin 100 sanaa)
  kirjoituksen tärkeimmästä sisällöstä: mitä ja miten on tutkittu,
  sekä mitä tuloksia on saatu. 
  Tiivistelmässä on lyhyt selvitys (noin 100 sanaa)
  kirjoituksen tärkeimmästä sisällöstä: mitä ja miten on tutkittu,
  sekä mitä tuloksia on saatu. 
\end{abstractpage}

%% Preface
\mysection{Preface}

This thesis work was carried out at the Microsoft headquarters at the campus in Redmond, USA.
First I would like to thank...

% thank professor
% thank advisors for helping
% thank the team mates
% thank the company for the opportunity
% thank kara for unending support
% thank other friends

\vspace{5cm}
Espoo, y.y.2017

\vspace{5mm}
{\hfill Teemu T.\ Vartiainen \hspace{1cm}}

%% Force new page after preface
\newpage


%% Table of contents. 
\thesistableofcontents


%% Symbols and abbreviations (Abbreviations and definitions?)
\mysection{Symbols and Abbreviations placeholder}

\subsection*{Symbols}

\begin{tabular}{ll}
$\emptyset$  & empty set \\
\end{tabular}

\subsection*{Operators}

\begin{tabular}{ll}
$A \cup B$    & union of sets $A$ and $B$ \\
\end{tabular}

\subsection*{Abbreviations}

\begin{tabular}{ll}
MBI         & medium business intelligence \\
MSE         & mean square error \\
PII         & personally identifiable information \\
SLA         & service level agreement \\
SQL         & structured query language \\
\end{tabular}


%% Tweaks the page numbering to meet the requirement of the thesis format:
%% Begin the pagenumbering in Arabian numerals (and leave the first page
%% of the text body empty, see \thispagestyle{empty} below).
%% Additionally, force the actual text to begin on a new page with the 
%% \clearpage command.
%% \clearpage is similar to \newpage, but it also flushes the floats (figures
%% and tables).
%% There is no need to change these
%%
\cleardoublepage
\storeinipagenumber
\pagenumbering{arabic}
\setcounter{page}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Content starts here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\thispagestyle{empty}

\subsection{Motivation}
%       Importance of accurate real time data
\nyi{Still somewhat placeholder-y, needs sources?} \\

Having diagnostics and real-time data is important in the modern software operations.
Dashboards, visualizations, and interactive logging is becoming more and more important
in the fast-changing digital world. Often these solutions are labeled Business Intelligence.
Having an accurate sense of current system status and low response time to faults can provide a clear advantage
in business against competitors. \nyi{citation needed}

%       Importance of having both a big picture and detailed view of current processes
%         Information tailored to different people

In software operations, many stakeholders are interested in this information. 
These stakeholders come from different backgrounds with different kinds of expertise.
The stakeholders can include for example developers, system administrators, project managers, and partner representatives.
This means that not all people interested in the information have the same level of technical ability to 
interpret for example raw system logs. Additionally, there may be issues of confidentiality and
controlling who can see which data is often necessary.

Furthermore, different people are interested in different granularities of information.
The engineers have different needs for what they need to see compared to higher level managers
or partner representatives.
Thus, having customizable views for this data can be crucial,
since the users want to see the information that helps them in their jobs, nothing less and nothing more.

%       Predicting future events to give information and schedule tasks
In addition to views to the present, a peek to the future can provide a major advantage.
Being able to predict what happens soon in the future helps with scheduling tasks and lets people react 
to events before they even happen. With technologies like machine learning we can learn patterns in data and 
start estimating information beyond the present time.

%       Loosely coupling diagnostics from systems
%       Adapting to change in agile and quickly changing environment
Another aspect of software operations is that they are ever-changing. 
Today's bleeding edge is tomorrow's obsolete. A tightly coupled diagnostics system
becomes useless when the system or a workflow changes significantly. Any analytics or visualization solution should
be agnostic to what the system does in reality and what specific technologies it uses. 
This keeps the solution relevant for a long lifetime, 
and enables it to be useful for different systems in different environments. 
To save costs and time, the solution should adapt to change seamlessly without constant maintenance from the users or the system administrators. However, unintended changes should be detected and notified about.

%   not as easy as it seems
However, modern software operations all these goals form a challenge.
Complex, distributed, and interconnected systems may make this kind of monitoring complex.

\subsection{Microsoft}
%        store with tons of products and parallel workflows ongoing continuously
At a multinational company as large as Microsoft, the scale of information is staggering.
Distributed systems process data and generate log entries concurrently in the numbers of terabytes.
No single person can monitor all the logs from even a single system without the help of automated
graphs, alerts, and notifications.  

% relate this into the store
One of these systems is the Microsoft Windows store. The store contains hundreds of thousands of products
such as apps and games. Publishers such as software companies and individual developers submit thousands of 
products in the store each day. Before the products reach the public retail store, they need to be processed
through a pipeline of multiple steps. These steps can include collecting data, validating the package integrity, and
other automated and manual checks to make sure the product should be allowed in the store, 
and that the store systems have all the information they need to function.
Each product submission produces log entries to record what has happened in the past and what is happening right now.

%        many different users need different kinds of information
%        third party developers and customers want to know what is going on
These system log entries are collected and stored as they happen. 
Different stakeholders from software developers and engineers to third-party publishers and retail users 
may benefit from this data. However, the needs for each of them are
different with regards to detail and types of information.
The engineers often need very detailed information from all the processes and parts of the system to debug an issue real-time,
while a third party submitting a product may only be concerned on the big picture of whether their 
product is available in the retail store or not.

Furthermore, issues of privacy and confidentiality further complicate this issue.
For example, the detailed information the engineers need is often confidential to Microsoft or its partners.
Similarly, the high-level information shown to third party publishers is still private to them 
and can only be shown for the products that they own.
This information is business-critical when Microsoft is dealing with high profile publishers such as big game companies.

%            we need to be able to answer and provide a good service
Like any complex and ever-changing system, at any time some part of the system may end up in a fault
which causes the system to function against the expected and correct flow.
When such an issue arises, engineers and managers need to have tools to investigate what is happening. 

The store pipeline consists of multiple separate systems and workflows processing the incoming products.
If such a workflow does not have detailed monitoring functionality, 
finding the details to investigate an issue prove to be challenging.
This kind of workflow is, in essence, a black box system, since for an outside observer there is no information
about the internal state, only whether the workflow is running or it has been completed.
In addition, the duration of these workflows can vary greatly from seconds to hours to days. 

%        detecting failures is essential to keep up SLAs and good experience
%            automation, prioritization, customization
When running the store, Microsoft is providing a service to retail customers and product publishers.
To provide good service, the goal is to be transparent enough that the store operations to not hinder the
business of the customers. Providing accurate information and time estimations to the publishers
not only helps Microsoft internally, but also helps the publishers' business.
In addition, Microsoft must adhere to contracts such as
service level agreements (SLA). These contracts determine, among other metrics, 
maximum response times and lengths for various processes.
For example, there is an SLA for how long a product submission can take from the beginning to when it is available in the store.
Prioritizing tasks and notifying about faults as early as possible allows Microsoft to provide a good service. 
Furthermore, these notifications should be highly customizable so they go to the right people at the right time.

This is where an intelligent logging, visualization, and notification tool is needed. 
A solution that provides both the big picture and the details will help investigate issues faster and
provide good service to third party publishers and customers.

\subsection{Research goals}
%		Questions and elaboration
%       Scope and main concepts of research
In this thesis I investigated methods of monitoring and visualizing the product submission workflows in the Microsoft Windows store.
These workflows are triggered based on real world events, often by developers submitting applications and games to the store.
I investigated ways of analyzing these workflows in real time and providing useful information to different users with different needs. 

I carried out research and development at Microsoft to answer the following research questions:

\begin{enumerate}[label=RQ\arabic*]
    \item How can a real-time event log from multiple sources and multiple concurrent workflows be dynamically transformed into a directed graph that describes the process? 
    \item How can the graph and past log data be used to predict future events and their times?
    \item How can the predictions be used to detect anomalies in new events (or lack thereof)?
\end{enumerate}

For the first question (RQ1), the goal was to have a dynamic and unsupervised monitoring system 
that uses the current and past logs to automatically determine all the actors, 
workflows, and steps related to the product submissions.
The store pipeline is constantly being developed further to be faster and to include more functionality.
The system should require as little maintenance as possible, retraining itself automatically and often to adapt to change.
In addition, the project should include as little hardcoded configuration as possible.

The second goal for the project was the capability to estimate future events (RQ2). 
Many stakeholders are interested in the pipeline completion times, so they can schedule their tasks accordingly.
Thirdly, comparisons between the estimations and the realized event times should be used to notify 
about any anomalies, faults, or delays in the pipeline (RQ3).

A further goal for the project developed in this thesis was that it should not be tightly coupled to the 
specific submission workflows in the store. The solution should not depend on the specific activities so
it requires less maintenance and could be reused in other systems.

\subsection{Structure of the Thesis}
%   d. Structure of thesis
This thesis is divided in ... chapters. Chapter x explains ... and after that ... 
\nyi{(TBD)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% In a thesis, every section starts a new page, hence \clearpage
\clearpage
\section{Background}
\label{sec:background}

\subsection{Machine learning}

\nyi{This is a lame placeholder}

Machine learning is a term to describe algorithms that give computers the ability to learn from data without being explicitly programmed for that data. Supervised learning is a subset of machine learning. In supervised learning, the input for the computer is a set of training data with predefined labels. For example, the training data could be a set of emails each labeled with ''spam'' and ''not spam''. The algorithm uses this labeled data to learn patterns in the \emph{training phase}. After the training, computer can then use the information learned to label new unseen data.

% insert graph for traditional vs machine learning
\nyi{Insert the traditional vs machine learnin figure here}

In \emph{classification} tasks the training data consists of a training set of $N$ input vectors $\mathbf{x}^t$ and $N$ output values $r^t$.
$$\mathcal{D} = \{(\mathbf{x}^t, r^t)\}_{t=1}^N, \mathbf{x}^t \in \mathbb{R}^d, r^t \in \{0, 1\} $$
Here $d$ is the dimension of vectors $\mathbf{x}^t$ and $N$ is the number of samples in the training set.
The two classes are $C_0$ and $C_1$. $r^t = 1$ when $\mathbf{x}^t$ belongs to class $C_1$, $r^t = 0$ otherwise. \cite{alpaydin}
In \emph{regression} tasks the training data is very similar, with the exception of each desired value $r^t$ being a real number \cite{alpaydin}.
$$\mathcal{D} = \{(\mathbf{x}^t, r^t)\}_{t=1}^N, \mathbf{x}^t \in \mathbb{R}^d, r^t \in \mathbb{R}$$
In both cases the algorithms implement a function $g(\mathbf{x})$ which estimates the output parameter $r$ for a new vector $\mathbf{x}$. 
In classification $g(\mathbf{x}) = c$ where $c \in \{0, 1\}$. 
In regression $g(\mathbf{x}) = r$ where $r \in \mathbb{R}$.
The learners are evaluated over a separate test set.
$$\mathcal{D}_{test} = \{ (\mathbf{x}_*^t , r_*^t) \}_{t=1}^{N_{test}}$$
A metric such as the \emph{mean square error} (MSE) can be used as a score to evaluate the learner. \cite{alpaydin}
$$E(g | \mathcal{D}_{test}) = \frac{1}{N_{test}} \sum_{t=1}^{N_{test}} (r_*^t - g(\mathbf{x}_*^t))^2$$

\subsubsection{Regression}

% regression, theory overall (maybe move math stuff from above)
\nyi{Regression theory overall}

% list different models briefly
\nyi{Breifly introduce different models}

\subsubsection{Boosted decision trees}

\nyi{introduce the concept}

\nyi{introduce mathematics}

\nyi{talk about practical usage, limitations, etc}

\subsubsection{Poisson regression}

\nyi{introduction }

\nyi{mathematics}

\nyi{practical usage, limitations}

\subsection{Process mining}

\nyi{intro to process mining as a whole}

Process mining is a way of observing recorded events and extracting information from the underlying processes.
In this thesis I focus mostly on \emph{process discovery} which consists of finding the underlying process model for a list of events. 
This model can be expressed as a directed graph, which describes the process generating the event logs. 
The discovery is an exercise of dealing with log incompleteness and noise.
Incompleteness means the recorded events may not describe the full graph, especially when some 
sequences of event are more likely than others. 
The events also contain noise and inaccuracy. \cite{van2013discovering}

\nyi{talk about process discovery}

\subsubsection{Petri nets}
\nyi{petri nets}

\subsubsection{\textalpha-algorithm}
\nyi{alpha algorithm}

\subsubsection{Other methods?}
\nyi{talk about other methods for processes ?}

% - Anomaly detection ??
% - something else?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Related work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Related work}
\label{sec:relatedwork}

\nyi{Anomaly detection} \cite{bezerra2009anomaly}.

\nyi{What else...?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Problem statement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Problem Statement}
\label{sec:problem}

%% intro
In this section I will describe the distributed product ingestion and publishing system examined in this thesis (later: \textit{system})  and elaborate on the 
problems that I was solving. I will also describe the users of the project developed in this thesis (later: \textit{project}) and what the needs for
each of them were. \nyi{...}

%% description of the store
%% ingestion system
%% workflows, processors, parallelism
%% description of the terms used
The system being investigated in this thesis was the Microsoft Windows store.
The store sells digital \emph{products} such as games and applications.
Each product belongs to a \emph{publisher} such as an independent developer or a game company.
A product is \emph{submitted} by a publisher as a digital application package.
Before it is \emph{published} to the store catalog, it needs to be \emph{ingested},
which means the package is verified and processed through multiple steps.
After the package has been ingested and the necessary information has been collected,
it can be published, which also involves a pipeline of steps.
This involves an ingestion-publishing \emph{workflow} that consists of multiple \emph{activities} (steps).
Completion of each of these activities is logged as an \emph{event}.
The list of events for a single submission forms a \emph{trace}.

% describe: workflow, trace (submission), product (bigid), publisher, event
\begin{table}[htb]
\begin{center}
\begin{tabularx}{\linewidth}{| l | X |} \hline
Product   & Single application package being processed in the system. \\ \hline
Publisher & Developer or a company submitting a product into the store. \\ \hline
Activity  & Single step of the workflow where some part of the product is processed. \\ \hline
Processor & Part of the system that executes a specific activity. \\ \hline
Workflow  & Abstract description of the whole pipeline consisting of a number of sequential and parallel activities. It contains the whole set of activities and their dependencies. \\ \hline
Submission & Single execution of the workflow for a single product.\\ \hline
Event     & Log entry documenting when a specific execution of an activity has finished or changed status. \\ \hline
Trace     & Single set of events describing current or past submission. 
            A trace is tied to a specific product from a specific publisher and has information about all 
            the activities and their execution times. \\ \hline
\end{tabularx}
\end{center}
\caption{Terms used in this thesis}
\label{tab:termdefinitions}
\end{table}

%% logging
These workflows are processed in a distributed system as a pipeline of steps. 
Multiple concurrent submissions are in progress at any moment of time.
Furthermore, many steps of the workflow are independent of each other.
Thus, a single submission can have multiple steps in progress at the same time.
These steps are often handled by an ''aggregator'' step which waits for 
all the parallel steps to finish before continuing the workflow process.
The different steps of the workflow have different durations that differ based on
the step itself, the characteristics of the package, and the current workload of the system.
Because of these uncertainties, traces from two different submissions may differ from each other, 
since the completion order of the parallel steps is unknown.

\nyi{Insert graph displaying a workflow with sequential and concurrent steps}

When each step completes, an event is generated.
These events are collected from the different processors into a single log database.
Each step is associated with a timestamp and metadata from the event and the submissions.
The system works with a ''best effort'' delivery, which means the delivery of the events to the log database
may have undetermined delays. Furthermore, the distributed system involves multiple machines
in multiple locations. This results in variance of seconds to minutes in the clocks of the systems.
The clock variance is directly seen as noise in the timestamps of the events.

Because of the parallelism and uncertainties mentioned the overall state of the system is difficult to describe at any given time. At the time of writing the system produced on average 15~000 events per hour with peak times averaging in the 30~000 range. For the publisher of a product the system is essentially a ''black box'' where
they cannot see the internal status, only whether the workflow has been completed or if it is still in progress.

\subsection{Requirements}

%% description of users
% engineers, developers, working on ingestion
% first party and third party publisher release managers
% managers, PMs
To understand the purpose of the project described in this thesis, we need to look at the users of the system
and what are their needs regarding to the project. There were four user groups relevant to the project.

\textbf{Developers}  are the software engineers working on the ingestion and publishing system.
\textbf{Managers}  are the developer leads and program managers who coordinate the developer time and what they are working on.
\textbf{Publishers} are the independent developers and companies submitting the packages to the store.
\textbf{Release managers} are the contacts on Microsoft's side who communicate with the large publishing companies who develop the ''triple-A'' games and applications.
\textbf{Manual reviewers} are the people working for Microsoft that do the manual steps of the product validation when necessary. This includes for example checking for fraud or inappropriate graphics or language.
The needs for each user group are covered in table \ref{tab:userneeds}.
I used the ''user story'' format for documenting the needs.
%% description that user needs were not fully known so they need engineering

It should be noted that the user needs were not fully understood in the beginning. The project was done in two cycles, with some necessary requirements engineering done in the beginning of both cycles. See section \ref{sec:timeline} for the project timeline. 
The user needs found at the start of the second section were related mostly on the presentation and the user interface, so they did not affect the main structure of the project significantly.

%% description of initial user needs
% which?

\begin{table}[htb]
\begin{center}
\begin{tabularx}{\linewidth}{| l | X |}
\hline
\textbf{User group} & \textbf{User stories} \nyi{TODO: format as user stories} \\
\hline \hline
\textbf{Developers} & 
- Need to see the current status of a submission for investigating issues with a single submission or product \newline 
- Need to see the shape of the workflows to find issues like race conditions\newline
- Need to see statistics to report average times \newline
- Need to be notified of any anomalies in the workflow to investigate issues faster\newline
- Need to be able to customize their views to drill down to the specific events they care about\\
\hline
\textbf{Managers} & 
- Need statistics of the workflows to report the system performance \\
\hline
\textbf{Publishers} & 
- Need to be able to know estimated times for submission completion \newline
- Need to be able to inquire about the status of their submissions \\
\hline
\textbf{Release managers} &
- Need to see the detailed status of a single product or submission \newline
- Need to see the big picture status quickly for all submissions related to a single product or publisher \newline
- Need to be notified about completion of crucial steps of the workflow or any issues \newline
- Need to be able to customize the views to show the submissions they are interested in \\
\hline
\textbf{Manual reviewers} & 
- Need to be notified in advance when a product is heading towards a manual review to schedule their workload more efficiently \\
\hline
\end{tabularx}
\end{center}
\caption{Initial user needs found in January \nyi{is this all?}}
\label{tab:userneeds}
\end{table}

%% business requirements
%% need to integrate with an existing system
%% confidentiality, integrity
%% working with partners

There were also several business requirements from Microsoft's side for this project. They can be summarized as integration with existing systems and following confidentiality requirements. \nyi{something else?}
The project was required to integrate with the existing store backend systems. 
The event collection and storing was already handled by a system called Jury, which is an interface to browse the products in the store and see diagnostics information.
The existing system stored the events in a database and allowed the used to query the events with a SQL-like query language. The results were shown as a list of rows with the matching events and timestamps. The project was to integrate with this querying system to load the events from the database.
The events and the product metadata in the system contained confidential information. Mainly there were two terms used to classify this. First term was \textit{Medium Business Intelligence} (MBI) and the more strict was \textit{Personally Identifiable Information} (PII). In practice it meant that PII-classified information should not be visible through the interface provided by the project, and the MBI-classified information related to a product should only be shown to the publisher or the partner who owns the product. However, the developers working at Microsoft should be able to see all MBI-classified information.

%% challenges
%%  user needs not fully understood
%%  unknown parallelism
%%  unsupervised learning and adaptation
%%  both real time and statistical data are needed

There were several challenges discovered in the beginning of the project.
The system implemented in the project should be unsupervised.
This means that the system should use past data to build an understanding of the workflows, without the need for a user to supply any knowledge beforehand.
The system should adapt to any changes in the workflow automatically.
This means that the distributed workflows contain unknown parallelism that must be detected automatically.
It was also discovered that the distributed system contains noise in the timestamps which further complicate the parallelism detection.
In addition, the system should be able to provide two different kinds of information. The workflow models and statistics should be build based on a long term aggregate discovered from several days worth of data, but the system should also show the real time data for the current submissions.
These two sides of information should both be utilized in the models.
Lastly, the user needs needed some requirements engineering. This is why an iterative process was set up. Section \ref{sec:timeline} contains a detailed description of this process.

%% needs discovered at meetings?

%% conclusion, key goals
To recap, the key goals and values for the solution are the ability to dynamically adapt to changes in the workflow,
the ability for the user to customize the information they need, an decoupling the solution from the specific 
workflow steps of the store.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Methods
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Research Material and Methods}
\label{sec:methods}

%T\"ass\"a osassa kuvataan k\"aytetty tutkimusaineisto ja
%tutkimuksen metodologiset valinnat, sek\"a
%kerrotaan tutkimuksen toteutustapa ja k\"aytetyt menetelm\"at. 

\subsection{Project timeline}
\label{sec:timeline}
% description of research timeline
% weekly meetings with manager, PM
While the main goal of the project was established as the project started in early January,
the details needed some research. For this reason, an iterative process was set up.
I held weekly meetings with my manager, as well as weekly project meetings together with my mentor, my manager, and the team program manager. In addition, in the mid-way point I held presentations to the end-users of the project to find out more requirements and to get feedback. Because of this, the project was divided in two iterations.
The weekly meetings were carried out through both iterations.

The project was carried out as follows:
\textbf{November -- December}: Preparation and background research.
\textbf{January -- Mid-February}: First iteration.
\textbf{Mid-February}: User meetings.
\textbf{Mid-February -- March}: Second iteration.
\textbf{End of March}: Project end and delivery.
\nyi{Format this as a figure}

% - first iteration: supporting work, backend, graphing, initial UI
%   - explorative prototypes
%   - two UI proposals, graph and timeline
I started the first iteration by building a prototype using a small constant dataset of events exported from the database. The purpose of the prototype was to explore the graph generation algorithms and to build a proof of concept. After the prototype was validate with the team I integrated it to the Jury system. I built two user interface proposals based on different types of presentation. The first one showed a directed graph describing the workflow model. The second interface showed the events on a scrollable timeline.

% - user meetings (brownbag, 3pp)
After the first iterations I held three presentations, two of which included a feedback session with exploration for use cases. The first presentation was an open one to the higher management and the other teams. The second presentation was with the release managers to get feedback from them and to find more requirements. The third was a "brown bag" type of open meeting with other team members who interact with the Jury system.
In these meetings I validated my prototype designs and collected more requirements.

% - second iteration: user interface work, features to fit needs
%   - machine learning exploration
%   - graphing accuracy exploration (alt graph)
In the second iteration I spent more effort on the user interface, based on the meetings.
I experimented with machine learning models and the graphing algorithm.
Furthermore, I worked with the team to integrate the graphs with the email notification system in Jury so they could be used to send emails about delays observed in the system. 
By the end of the second iteration the project was fully integrated with Jury and was delivered to the users.

\subsection{Data}
% description of event hub
The data used by the project is a list of events. The events work as log entries generated by the distributed processes of the store ingestion/publishing system. Each store system creates events and sends them through an Azure Event Hub. 

Azure Event Hubs are a common event streaming platform that works as a platform-independent "front door" for all the events \cite{eventhubs}. All the processors of the store systems broadcast events into a common Event Hub.
The events are then read and processed by Jury and stored in a SQL database.

% specific description of events and how they are stored
Each event consists of a \textbf{timestamp}, identifying fields, and some metadata. The event timestamp describes the exact time the event happened. There are also three reference fields referencing the \textbf{submission}, the \textbf{product}, and the \textbf{publisher} that the event belongs to. In addition to these, the three idenfier fields relevant in this research are textual (string-type) fields \textbf{Source}, \textbf{Subsource}, and \textbf{Status}. The Source field identifies the even source system, such as ''Ingestion'', whereas the Subsource identifies the exact processor step withing that system. Thus, the Source--Subsource pair is enough to identify the exact step of the workflow. The Status field contains a string describing the status of the step, such as ''Completed'', ''In progress'' or ''Failed''.

\nyi{event as a figure?}

% methods explaining splitting and grouping of traces
Thus, an arbitrary set of events can be grouped into traces by the submission field. Within the grouping each individual step can then be identified by the Source--Subsource pair.

\nyi{figure of events grouped into traces}

\subsection{Graph construction}
\nyi{constructing graphs}\\
\nyi{these all on theoretical level}

\nyi{- alpha algorithm}\\
\nyi{- simplified petri-net}\\
\nyi{- dealing with noise, possible sources}\\
\nyi{- option/support for forced graph shape}\\
\nyi{- mention queries and customization}

\nyi{collecting statistics}\\
\nyi{- as graph is being generted}\\
\nyi{- with a predetermined shape}

\subsection{Real-time functionality}
\nyi{overlaying real-time data}\\
\nyi{timeline generation}

\subsection{Estimating the future}
\nyi{estimating future}\\
\nyi{- - statistical approach TP75 etc}

\subsection{Machine learning}
\nyi{ML approach}\\
\nyi{explain models used (poisson, bdt) and reasoning why these}

\nyi{explain training dataset}

\nyi{tests with different features}\\
\nyi{ identical posting}\\
%  posting that needed human interventiondcxfkkkkkkkkkkkkkkkkkkm
%                                       a cat did this ^ 
\nyi{posting that needed human intervention}

\subsubsection{Azure ML}
\nyi{explain briefly what azure ML is}

\subsection{Notifications}
\nyi{describe how the estimates could be used for notifications}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Implementation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Implementation Details}

\nyi{some introduction}

\nyi{integration with the existing system}

\nyi{sql queries, c\# linq to sql, combined with text query}\\
\nyi{talk about customized views}\\
\nyi{everything based on user-made queries, even templates}

\nyi{details about the simple footprint generation}\\
\nyi{include tables from observed noise}

\nyi{turning the footprint into a graph}

\nyi{maybe: talk about implementing the alternative graph}

\nyi{option, read graph from definition file}

\nyi{storing the graph, caching}

\nyi{overlaying real-time data with the model}

\nyi{estimate based on collected statistics}\\
\nyi{statistics}\\
\nyi{machine learning models}

\nyi{constructing timeline from the graph}

\nyi{user interface}

\nyi{mention difficulties (or write them down somewhere at least)}\\
\nyi{you'll want these for the presentation}\\
\nyi{ parallelism}\\
\nyi{ clock skew}\\
\nyi{ changes in workflow}\\
\nyi{ bugs manifesting in graphs (is this really a bad thing?)}\\
\nyi{ dealing with rare events (manual review)}\\
\nyi{  ask tao whether event types and such are confidential}

\nyi{vis js, asp.net mvc, etc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Results}
\label{sec:results}

%T\"ass\"a osassa esitet\"a\"an tulokset ja vastataan tutkielman alussa
%esitettyihin tutkimuskysymyksiin. Tieteellisen kirjoitelman
%arvo mitataan t\"ass\"a osassa esitettyjen tulosten perusteella.

\nyi{graph generation}

\nyi{maybe: alternative graph failure because of noise and large variation}

\nyi{statistics and their accuracy (run stats on the same dataset)}

\nyi{ML models and their scores}

\nyi{user satisfaction}


\subsection{Evaluation}
\label{sec:evaluation}

%Tutkimustuloksien merkityst\"a on aina syyt\"a arvioida ja tarkastella
%kriittisesti.  Joskus tarkastelu voi olla t\"ass\"a osassa, mutta se
%voidaan my\"os j\"att\"a\"a viimeiseen osaan, jolloin viimeisen osan nimeksi
%tulee >>Tarkastelu>>. Tutkimustulosten merkityst\"a voi arvioida my\"os
%>>Johtop\"a\"at\"okset>>-otsikon alla viimeisess\"a osassa. 

%T\"ass\"a osassa on syyt\"a my\"os arvioida tutkimustulosten luotettavuutta.
%Jos tutkimustulosten merkityst\"a arvioidaan >>Tarkastelu>>-osassa,
%voi luotettavuuden arviointi olla my\"os siell\"a. 

\nyi{talk about success with graph generation}\\
\nyi{maybe talk about alt graph?}\\
\nyi{finding bugs in system or faults based on graphs}\\
\nyi{good feedback from users}

\nyi{talk about negatives with machine learning models}\\
\nyi{reason why they would turn out so bad}

\nyi{talk about how the system went immediately into production}\\
\nyi{talk about the success of notifications built on top of the system}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Conclusions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Conclusions and Future Work} 
\label{sec:conclusions}

\begin{itemize}
\item[--]\nyi{Contributions (positives)}
\item[--]\nyi{Limitations (negatives)}
\item[--]\nyi{Future work}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
%% The \phantomsection command is necessary for hyperref to jump to the 
%% correct page, in other words it puts a hyper marker on the page.

\phantomsection
\addcontentsline{toc}{section}{References}

\printbibliography

%% Appendices
%\clearpage
%\thesisappendix

\end{document}
