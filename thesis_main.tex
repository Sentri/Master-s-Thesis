%% using pdflatex, which directly typesets your document in
%% pdf (use jpg or pdf figures)
\documentclass[english,12pt,a4paper,pdftex,sci,utf8]{aaltothesis}
%% To the \documentclass above
%% specify your school: arts, biz, chem, elec, eng, sci
%% specify the character encoding scheme used by your editor: utf8, latin1

\usepackage{graphicx}

%% Use this if you write hard core mathematics, these are usually needed
\usepackage{amsfonts,amssymb,amsbsy}

\usepackage{tabularx}
\usepackage{csquotes}
\usepackage{biblatex}
\usepackage{enumitem}

\addbibresource{bibliography.bib}

%% Use the macros in this package to change how the hyperref package below 
%% typesets its hypertext -- hyperlink colour, font, etc. See the package
%% documentation. It also defines the \url macro, so use the package when 
%% not using the hyperref package.
%%
%\usepackage{url}

%% Use this if you want to get links and nice output. Works well with pdflatex.
\usepackage{hyperref}
\PackageWarning{NYI}{Remember to edit this}
\hypersetup{pdfpagemode=UseNone, pdfstartview=FitH,
  colorlinks=true,urlcolor=red,linkcolor=blue,citecolor=black,
  pdftitle={Default Title, Modify},pdfauthor={Teemu Vartiainen},
  pdfkeywords={Modify keywords}}

% custom command for edit comments
\definecolor{nyitext}{rgb}{0.2, 0.2, 0.2}
\definecolor{nyibg}{rgb}{1.0, 1.0, 0.6}
\newcommand{\nyi}[1]{\colorbox{nyibg}{\textcolor{nyitext}{\emph{#1}}}}

%\PackageWarning{NYI}{#1}

%% All that is printed on paper starts here
\begin{document}

%% ONLY FOR M.Sc. AND LICENTIATE THESIS: Specify your department,
%% professorship and professorship code. 
%%
\department{Department of Computer Science}
\professorship{x}
%%


%% Choose one of these:
\univdegree{MSc}

%% Your own name (should be self explanatory...)
\author{Teemu Vartiainen}

%% Your thesis title comes here and again before a possible abstract in
%% Finnish or Swedish . If the title is very long and latex does an
%% unsatisfactory job of breaking the lines, you will have to force a
%% linebreak with the \\ control character. 
%% Do not hyphenate titles.

% working title
\thesistitle{Analyzing real-time and past events for describing processes and detecting anomalies}

\place{Espoo}

%% For B.Sc. thesis use the date when you present your thesis. 
%% 
%% Kandidaatintyön päivämäärä on sen esityspäivämäärä! 
\date{x.x.2017}

%% B.Sc. or M.Sc. thesis supervisor 
%% Note the "\" after the comma. This forces the following space to be 
%% a normal interword space, not the space that starts a new sentence. 
%% This is done because the fullstop isn't the end of the sentence that
%% should be followed by a slightly longer space but is to be followed
%% by a regular space.
%%
\supervisor{Prof.\ Petri Vuorimaa}

%% B.Sc. or M.Sc. thesis advisors(s). You can give upto two advisors in
%% this template. Check with your supervisor how many official advisors
%% you can have.
%%
%\advisor{Prof.\ Pirjo Professori}
\advisor{M.Sc.\ Tao Zhu}
\advisor{M.Sc.\ Rafael Forsbach Valle}

%% Aalto logo: syntax:
%% \uselogo{aaltoRed|aaltoBlue|aaltoYellow|aaltoGray|aaltoGrayScale}{?|!|''}
%%
%% Logo language is set to be the same as the document language.
%% Logon kieli on sama kuin dokumentin kieli
%%
\uselogo{aaltoBlue}{?}

%% Create the coverpage
%%
\makecoverpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Abstracts & etc
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Note that when writting your master's thesis in English, place
%% the English abstract first followed by the possible Finnish abstract

%% English abstract.
%% All the information required in the abstract (your name, thesis title, etc.)
%% is used as specified above.
%% Specify keywords
%%
%% Kaikki tiivistelmässä tarvittava tieto (nimesi, työnnimi, jne.) käytetään
%% niin kuin se on yllä määritelty.
%% Avainsanat
%%
\keywords{event logs, machine learning, process mining}
%% Abstract text
\begin{abstractpage}[english]
  Your abstract in English. Try to keep the abstract short; approximately 
  100 words should be enough. The abstract explains your research topic, 
  the methods you have used, and the results you obtained.  
  Your abstract in English. Try to keep the abstract short; approximately 
  100 words should be enough. The abstract explains your research topic, 
  the methods you have used, and the results you obtained.  

  Your abstract in English. Try to keep the abstract short; approximately 
  100 words should be enough. The abstract explains your research topic, 
  the methods you have used, and the results you obtained.  
  Your abstract in English. Try to keep the abstract short; approximately 
  100 words should be enough. The abstract explains your research topic, 
  the methods you have used, and the results you obtained.  
\end{abstractpage}

%% Force a new page so that the possible English abstract starts on a new page
\newpage

%% Abstract in Finnish.

\thesistitle{Reaaliaikaisten ja menneiden prosessitapahtumien k\"aytt\"o prosessien kuvaamisessa}
\supervisor{Prof.\ Petri Vuorimaa}
\advisor{M.Sc.\ Tao Zhu}
\advisor{M.Sc.\ Rafael Forsbach Valle}
\department{Tietotekniikan laitos}
\professorship{x}
%% Avainsanat
\keywords{Tapahtumalokit, koneoppiminen, prosessikaaviot}
%% Tiivistelmän tekstiosa
\begin{abstractpage}[finnish]
  Tiivistelmässä on lyhyt selvitys (noin 100 sanaa)
  kirjoituksen tärkeimmästä sisällöstä: mitä ja miten on tutkittu,
  sekä mitä tuloksia on saatu. 
  Tiivistelmässä on lyhyt selvitys (noin 100 sanaa)
  kirjoituksen tärkeimmästä sisällöstä: mitä ja miten on tutkittu,
  sekä mitä tuloksia on saatu. 

  Tiivistelmässä on lyhyt selvitys (noin 100 sanaa)
  kirjoituksen tärkeimmästä sisällöstä: mitä ja miten on tutkittu,
  sekä mitä tuloksia on saatu. 
  Tiivistelmässä on lyhyt selvitys (noin 100 sanaa)
  kirjoituksen tärkeimmästä sisällöstä: mitä ja miten on tutkittu,
  sekä mitä tuloksia on saatu. 
  Tiivistelmässä on lyhyt selvitys (noin 100 sanaa)
  kirjoituksen tärkeimmästä sisällöstä: mitä ja miten on tutkittu,
  sekä mitä tuloksia on saatu. 
\end{abstractpage}

%% Preface
\mysection{Preface}

This thesis work was carried out at the Microsoft headquarters at the campus in Redmond, USA.
First I would like to thank...

% thank professor
% thank advisors for helping
% thank the team mates
% thank the company for the opportunity
% thank kara for unending support
% thank other friends

\vspace{5cm}
Espoo, y.y.2017

\vspace{5mm}
{\hfill Teemu T.\ Vartiainen \hspace{1cm}}

%% Force new page after preface
\newpage


%% Table of contents. 
\thesistableofcontents


%% Symbols and abbreviations (Abbreviations and definitions?)
\mysection{Symbols and Abbreviations placeholder}

\subsection*{Symbols}

\begin{tabular}{ll}
$\emptyset$  & empty set \\
\end{tabular}

\subsection*{Operators}

\begin{tabular}{ll}
$A \cup B$    & union of sets $A$ and $B$ \\
\end{tabular}

\subsection*{Abbreviations}

\begin{tabular}{ll}
SLA         & service level agreement \\
SQL         & structured query language
\end{tabular}


%% Tweaks the page numbering to meet the requirement of the thesis format:
%% Begin the pagenumbering in Arabian numerals (and leave the first page
%% of the text body empty, see \thispagestyle{empty} below).
%% Additionally, force the actual text to begin on a new page with the 
%% \clearpage command.
%% \clearpage is similar to \newpage, but it also flushes the floats (figures
%% and tables).
%% There is no need to change these
%%
\cleardoublepage
\storeinipagenumber
\pagenumbering{arabic}
\setcounter{page}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Content starts here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\thispagestyle{empty}

\subsection{Motivation}
%       Importance of accurate real time data
\nyi{Still somewhat placeholder-y, needs sources?} \\

Having diagnostics and real-time data is important in the modern software operations.
Dashboards, visualizations, and interactive logging is becoming more and more important
in the fast-changing digital world. Often these solutions are labeled Business Intelligence.
Having an accurate sense of current system status and low response time to faults can provide a clear advantage
in business against competitors. \nyi{citation needed}

%       Importance of having both a big picture and detailed view of current processes
%         Information tailored to different people

In software operations, many stakeholders are interested in this information. 
These stakeholders come from different backgrounds with different kinds of expertise.
The stakeholders can include for example developers, system administrators, project managers, and partner representatives.
This means that not all people interested in the information have the same level of technical ability to 
interpret for example raw system logs. Additionally, there may be issues of confidentiality and
controlling who can see which data is often necessary.

Furthermore, different people are interested in different granularities of information.
The engineers have different needs for what they need to see compared to higher level managers
or partner representatives.
Thus, having customizable views for this data can be crucial,
since the users want to see the information that helps them in their jobs, nothing less and nothing more.

%       Predicting future events to give information and schedule tasks
In addition to views to the present, a peek to the future can provide a major advantage.
Being able to predict what happens soon in the future helps with scheduling tasks and lets people react 
to events before they even happen. With technologies like machine learning we can learn patterns in data and 
start estimating information beyond the present time.

%       Loosely coupling diagnostics from systems
%       Adapting to change in agile and quickly changing environment
Another aspect of software operations is that they are ever-changing. 
Today's bleeding edge is tomorrow's obsolete. A tightly coupled diagnostics system
becomes useless when the system or a workflow changes significantly. Any analytics or visualization solution should
be agnostic to what the system does in reality and what specific technologies it uses. 
This keeps the solution relevant for a long lifetime, 
and enables it to be useful for different systems in different environments. 
To save costs and time, the solution should adapt to change seamlessly without constant maintenance from the users or the system administrators. However, unintended changes should be detected and notified about.

%   not as easy as it seems
However, modern software operations all these goals form a challenge.
Complex, distributed, and interconnected systems may make this kind of monitoring complex.

\subsection{Microsoft}
%        store with tons of products and parallel workflows ongoing continuously
At a multinational company as large as Microsoft, the scale of information is staggering.
Distributed systems process data and generate log entries concurrently in the numbers of terabytes.
No single person can monitor all the logs from even a single system without the help of automated
graphs, alerts, and notifications.  

% relate this into the store
One of these systems is the Microsoft Windows store. The store contains hundreds of thousands of products
such as apps and games. Publishers such as software companies and individual developers submit thousands of 
products in the store each day. Before the products reach the public retail store, they need to be processed
through a pipeline of multiple steps. These steps can include collecting data, validating the package integrity, and
other automated and manual checks to make sure the product should be allowed in the store, 
and that the store systems have all the information they need to function.
Each product submission produces log entries to record what has happened in the past and what is happening right now.

%        many different users need different kinds of information
%        third party developers and customers want to know what is going on
These system log entries are collected and stored as they happen. 
Different stakeholders from software developers and engineers to third-party publishers and retail users 
may benefit from this data. However, the needs for each of them are
different with regards to detail and types of information.
The engineers often need very detailed information from all the processes and parts of the system to debug an issue real-time,
while a third party submitting a product may only be concerned on the big picture of whether their 
product is available in the retail store or not.

Furthermore, issues of privacy and confidentiality further complicate this issue.
For example, the detailed information the engineers need is often confidential to Microsoft or its partners.
Similarly, the high-level information shown to third party publishers is still private to them 
and can only be shown for the products that they own.
This information is business-critical when Microsoft is dealing with high profile publishers such as big game companies.

%            we need to be able to answer and provide a good service
Like any complex and ever-changing system, at any time some part of the system may end up in a fault
which causes the system to function against the expected and correct flow.
When such an issue arises, engineers and managers need to have tools to investigate what is happening. 

The store pipeline consists of multiple separate systems and workflows processing the incoming products.
If such a workflow does not have detailed monitoring functionality, 
finding the details to investigate an issue prove to be challenging.
This kind of workflow is, in essence, a black box system, since for an outside observer there is no information
about the internal state, only whether the workflow is running or it has been completed.
In addition, the duration of these workflows can vary greatly from seconds to hours to days. 

%        detecting failures is essential to keep up SLAs and good experience
%            automation, prioritization, customization
When running the store, Microsoft is providing a service to retail customers and product publishers.
To provide good service, the goal is to be transparent enough that the store operations to not hinder the
business of the customers. Providing accurate information and time estimations to the publishers
not only helps Microsoft internally, but also helps the publishers' business.
In addition, Microsoft must adhere to contracts such as
service level agreements (SLA). These contracts determine, among other metrics, 
maximum response times and lengths for various processes.
For example, there is an SLA for how long a product submission can take from the beginning to when it is available in the store.
Prioritizing tasks and notifying about faults as early as possible allows Microsoft to provide a good service. 
Furthermore, these notifications should be highly customizable so they go to the right people at the right time.

This is where an intelligent logging, visualization, and notification tool is needed. 
A solution that provides both the big picture and the details will help investigate issues faster and
provide good service to third party publishers and customers.

\subsection{Research goals}
%		Questions and elaboration
%       Scope and main concepts of research
In this thesis I investigated methods of monitoring and visualizing the product submission workflows in the Microsoft Windows store.
These workflows are triggered based on real world events, often by developers submitting applications and games to the store.
I investigated ways of analyzing these workflows in real time and providing useful information to different users with different needs. 

I carried out research and development at Microsoft to answer the following research questions:

\begin{enumerate}[label=RQ\arabic*]
    \item How can a real-time event log from multiple sources and multiple concurrent workflows be dynamically transformed into a directed graph that describes the process? 
    \item How can the graph and past log data be used to predict future events and their times?
    \item How can the predictions be used to detect anomalies in new events (or lack thereof)?
\end{enumerate}

For the first question (RQ1), the goal was to have a dynamic and unsupervised monitoring system 
that uses the current and past logs to automatically determine all the actors, 
workflows, and steps related to the product submissions.
The store pipeline is constantly being developed further to be faster and to include more functionality.
The system should require as little maintenance as possible, retraining itself automatically and often to adapt to change.
In addition, the system should include as little hardcoded configuration as possible.

The second goal for the system was the capability to estimate future events (RQ2). 
Many stakeholders are interested in the pipeline completion times, so they can schedule their tasks accordingly.
Thirdly, comparisons between the estimations and the realized event times should be used to notify 
about any anomalies, faults, or delays in the pipeline (RQ3).

A further goal for the solution developed in this thesis was that it should not be tightly coupled to the 
specific submission workflows in the store. The solution should not depend on the specific activities so
it requires less maintenance and could be reused in other systems.

\subsection{Structure of the Thesis}
%   d. Structure of thesis
This thesis is divided in ... chapters. Chapter x explains ... and after that ... 
\nyi{(TBD)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% In a thesis, every section starts a new page, hence \clearpage
\clearpage
\section{Background}
\label{sec:background}

\subsection{Machine learning}

\nyi{Broad intro to machine learning}

Machine learning is a term to describe algorithms that give computers the ability to learn from data without being explicitly programmed for that data. Supervised learning is a subset of machine learning. In supervised learning, the input for the computer is a set of training data with predefined labels. For example, the training data could be a set of emails each labeled with ''spam'' and ''not spam''. The algorithm uses this labeled data to learn patterns in the \emph{training phase}. After the training, computer can then use the information learned to label new unseen data.

% insert graph for traditional vs machine learning

In \emph{classification} tasks the training data consists of a training set of $N$ input vectors $\mathbf{x}^t$ and $N$ output values $r^t$.
$$\mathcal{D} = \{(\mathbf{x}^t, r^t)\}_{t=1}^N, \mathbf{x}^t \in \mathbb{R}^d, r^t \in \{0, 1\} $$
Here $d$ is the dimension of vectors $\mathbf{x}^t$ and $N$ is the number of samples in the training set.
The two classes are $C_0$ and $C_1$. $r^t = 1$ when $\mathbf{x}^t$ belongs to class $C_1$, $r^t = 0$ otherwise. \cite{alpaydin}
In \emph{regression} tasks the training data is very similar, with the exception of each desired value $r^t$ being a real number \cite{alpaydin}.
$$\mathcal{D} = \{(\mathbf{x}^t, r^t)\}_{t=1}^N, \mathbf{x}^t \in \mathbb{R}^d, r^t \in \mathbb{R}$$
In both cases the algorithms implement a function $g(\mathbf{x})$ which estimates the output parameter $r$ for a new vector $\mathbf{x}$. 
In classification $g(\mathbf{x}) = c$ where $c \in \{0, 1\}$. 
In regression $g(\mathbf{x}) = r$ where $r \in \mathbb{R}$.
The learners are evaluated over a separate test set.
$$\mathcal{D}_{test} = \{ (\mathbf{x}_*^t , r_*^t) \}_{t=1}^{N_{test}}$$
A metric such as the \emph{mean square error} (MSE) can be used as a score to evaluate the learner. \cite{alpaydin}
$$E(g | \mathcal{D}_{test}) = \frac{1}{N_{test}} \sum_{t=1}^{N_{test}} (r_*^t - g(\mathbf{x}_*^t))^2$$

\subsubsection{Regression}

\nyi{Regression theory}

% regression, theory overall (maybe move math stuff from above)

% list different models briefly

\subsubsection{Boosted decision trees}

% introduce the concept

% introduce mathematics

% talk about practical usage, limitations, etc

\subsubsection{Poisson regression}

% introduction 

% mathematics

% practical usage, limitations

\subsection{Process mining}

% intro to process mining as a whole

Process mining is a way of observing recorded events and extracting information from the underlying processes.
In this thesis I focus mostly on \emph{process discovery} which consists of finding the underlying process model for a list of events. 
This model can be expressed as a directed graph, which describes the process generating the event logs. 
The discovery is an exercise of dealing with log incompleteness and noise.
Incompleteness means the recorded events may not describe the full graph, especially when some 
sequences of event are more likely than others. 
The events also contain noise and inaccuracy. \cite{van2013discovering}

% talk about process discovery

% petri nets

% alpha algorithm

% talk about other methods for processes

% - Anomaly detection ??
% - something else?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Related work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Related work}
\label{sec:relatedwork}

Anomaly detection \cite{bezerra2009anomaly}.

What else...?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Problem statement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Problem Statement}
\label{sec:problem}

%% description of the store

%% ingestion system
%% workflows, processors, parallelism
%% description of the terms used
%% logging

%% description of users
% engineers, developers, working on ingestion
% first party and third party publisher release managers
% managers, PMs

%% description of initial user needs
% which?

%% description that user needs were not fully known so they need engineering

%% business requirements
%% need to integrate with an existing system
%% confidentiality, integrity
%% working with partners

%% challenges
%%  user needs not fully understood
%%  unknown parallelism
%%  unsupervised learning and adaptation
%%  both real time and statistical data are needed

%% needs discovered at meetings?

%% conclusion, key goals


\nyi{Needs a better intro paragraph}

% the store application ingestion
The system being investigated in this thesis was the Microsoft universal store and its workflows. 
The workflows consist of applications, games, and other products being ingested to the store backend systems and published to the store catalog.
The workflow starts when a real-world event (e.g. a developer submitting a new application) triggers it.
For the many of the users of the system (such as the application developer), the workflow works 
like a "black box" system. The workflow starts when it is triggered, and ends after some unknown time has passed.
The users have no way of getting more information than that the workflow is in progress and will finish some time 
in the future. This leaves the user in the dark. This is the problem this thesis is aiming to solve. 
\nyi{this is not a good way to say this}
\nyi{makes it seem like the whole store has no information to anyone}

% description about workflows
%  how they are similar but also different
%  multiple concurrent flows
%  different but unknown forks and parallelized steps
%  durations unknown
The submission workflow can be seen as a pipeline with a chain of multiple activities that depends on each other and have a sequence. 
Some activities within the workflow do not depend on each other so they can run in parallel.
Furthermore, multiple workflows run in parallel in the system.
They start and stop at undetermined times and their running times depend on the task at hand and the system load.\\

\nyi{Insert graph displaying a workflow with sequential and concurrent steps}\\

A workflow is triggered by a real-world event, usually by a user.
The user triggering a workflow is called the \emph{publisher}.
The workflow consists of \emph{activities} processing and validating a specific \emph{product} that the publisher owns.
The trigger happens when the publisher submits a product into the workflow.
The system executes the activities on physical workflow \emph{processors}.
Thus, a publisher can own multiple products and a product can go through a workflow multiple times.
In this thesis such instances of a single workflow process \emph{(submission)} are called a \emph{trace}.

In other words, a workflow describes the generic pipeline "shape" with all the activities and their dependencies.
A trace is an instance of a triggered workflow that is tied to a specific product from a specific publisher.
Furthermore, a trace contains more information related to the actual activities, such as a start time and an end time.
Such log entry of an execution of an activity is called an \emph{event}. Table \ref{tab:termdefinitions} lists all these terms that are used in this thesis.\\

\nyi{Insert graph describing the difference between workflow/activity and trace/event}\\

\nyi{Clarification}: A workflow is an abstract description of the thole pipeline consisting of a number of sequential and parallel activities. A publisher submits a product into the system, which triggers a new trace where activities are executes on processors. These executions are logged into the trace as events with timestamp and status information.
\nyi{embed this into the main text somehow}

% describe: workflow, trace (submission), product (bigid), publisher, event
\begin{table}[htb]
\begin{center}
\begin{tabularx}{\linewidth}{r | X}
Product   & A single real-world entity being processed in the pipeline. \\
Publisher & An agent such as a user who owns a product or multiple. \\
Activity  & A single step where some part of the product is processed or validated. \\
Processor & A part of the system receiving a product or its information that executes a specific activity. \\
Workflow  & A description of the whole set of activities and their dependencies. \\ 
Submission & A single triggered execution of the workflow for a single product.\\
Event     & A log entry documenting when a specific execution of an activity has finished or changes status. \\
Trace     & A single set of events describing current or past submission. 
            A trace is tied to a specific product from a specific publisher and has information about all 
            the activities and their execution times. \\
\end{tabularx}
\end{center}
\caption{Terms used in this thesis \nyi{Needs formatting and further clarification}}
\label{tab:termdefinitions}
\end{table}

% real time events from different sources
% listeners interpreting events
% master log containing everything
% log entries can be queried from this master log
% order unknown, somewhat accurate timestamps
At the time of writing the system produced on average 15~000 activity events per hour with peak times averaging in the 30~000 range. The events are created on workflow processors running on multiple systems. These events are collected and written to a relational database. 
The order of the arrival of events is not guaranteed to be the same as the order of the events really happening.
To combat this, each event contains its associated timestamp for when it was created. However, the different systems are
subject to clock skew, meaning the clocks of the systems may not be fully synchronized. 
The clocks may differ in the range of seconds to minutes.
After the events are received and written to the database, they can be queried.

% user needs
%  visualize processes without supervision or configuration
%  must adapt to changes in workflow
% different users concerned about different workflow steps or different entities
%  customizable/tailored views
%  ui level, user-level, no engineering effort
%  MBI, confidentiality
The user needs for the system include: \nyi{format}
\begin{itemize}
\item[--]visualize processes without supervision or configuration
\item[--]must adapt to changes in workflow
\item[--]different users concerned about different workflow steps or different products
\item[--]confidentiality concerns (MBI)
\end{itemize}

%  unknown parallelism involved
The solution developed needed to dynamically (unsupervised) generate a workflow description in the form of a directed graph
from the past log data. The log data consists of the event database. The database contains all the 
past and current traces. The traces are unorganized and interleaved in an random fashion. This is because many several
workflows can run in parallel and the delivery of events to the database is best-effort. Furthermore, since some parts of the workflow are concurrent instead of sequential, the order of events within a single trace can change. 
The solution should detect this parallelism automatically.

% two sides of the information
%  aggregate for past data
%  real-time info about current processes
The solution has to provide two types of information, the aggregate and the real-time. 
The aggregate information should contain statistics and the workflow graph generated from many days of event data.
The real-time data should reflect the current state of all the submissions in progress in the system.

% integration with an existing system
%  user interface
%  query language
%  authentication
The solution was to be integrated to an existing store management interface called Jury. 
Jury supports multiple users with various permission levels. Jury provides a query language to refine the information
that the user wants to see. The solution should integrate with the query language to allow users to customize 
what they see.

% Key goals/values
%  dynamic adaptation
%  customization (queries)
%  simplicity (speed)
%  decoupling (as little hardcoding for specific details as possible)

To recap, the key goals and values for the solution are the ability to dynamically adapt to changes in the workflow,
the ability for the user to customize the information they need, an decoupling the solution from the specific 
workflow steps of the store.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Methods
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Research Material and Methods}
\label{sec:methods}

%T\"ass\"a osassa kuvataan k\"aytetty tutkimusaineisto ja
%tutkimuksen metodologiset valinnat, sek\"a
%kerrotaan tutkimuksen toteutustapa ja k\"aytetyt menetelm\"at. 

\subsection{Project timeline}
% description of research timeline

% - first iteration: supporting work, backend, graphing, initial UI
%   - explorative prototypes
%   - two UI proposals, graph and timeline
% - user meetings (brownbag, 3pp)
% - second iteration: user interface work, features to fit needs
%   - machine learning exploration
%   - graphing accuracy exploration (alt graph)

\subsection{Data}
% description of event hub

% specific description of events and how they are stored

% methods explaining splitting and grouping of traces

\subsection{Graph construction}
% constructing graphs
% these all on theoretical level
% - dealing with noise, possible sources
% - alpha algorithm
% - simplified petri-net
% - option/support for forced graph shape
% - mention queries and customization

% collecting statistics
% - as graph is being generated
% - with a predetermined shape

\subsection{Real-time functionality}
% overlaying real-time data
% timeline generation

\subsection{Estimating the future}
% estimating future
% - - statistical approach TP75 etc

\subsection{Machine learning}
% ML approach
% explain models used (poisson, bdt) and reasoning why these

% explain training dataset

% tests with different features
%  identical posting
%  posting that needed human interventiondcxfkkkkkkkkkkkkkkkkkkm
%                                       a cat did this ^ 

\subsubsection{Azure ML}
% explain briefly what azure ML is

\subsection{Notifications}
% describe how the estimates could be used for notifications


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Implementation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Implementation Details}

\begin{itemize}
\item[--]Setup, DB, caching
\item[--]Graph construction
\item[--]Timeline construction
\item[--]Prediction
\item[--]ML experiments
\item[--]Anomalies/health detection
\end{itemize}

% some introduction

% integration with the existing system

% sql queries, c# linq to sql, combined with text query
% talk about customized views
% everything based on user-made queries, even templates

% details about the simple footprint generation
% include tables from observed noise

% turning the footprint into a graph

% maybe: talk about implementing the alternative graph

% option, read graph from definition file

% storing the graph, caching

% overlaying real-time data with the model

% estimate based on collected statistics
% statistics
% machine learning models

% constructing timeline from the graph

% user interface

% mention difficulties (or write them down somewhere at least)
% you'll want these for the presentation
%  parallelism
%  clock skew
%  changes in workflow
%  bugs manifesting in graphs (is this really a bad thing?)
%  dealing with rare events (manual review)
%   ask tao whether event types and such are confidential

% vis js, asp.net mvc, etc

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Results}
\label{sec:results}

%T\"ass\"a osassa esitet\"a\"an tulokset ja vastataan tutkielman alussa
%esitettyihin tutkimuskysymyksiin. Tieteellisen kirjoitelman
%arvo mitataan t\"ass\"a osassa esitettyjen tulosten perusteella.

\begin{itemize}
\item[--]Generated process graphs
\item[--]Prediction accuracy (numbers)
\item[--]ML scores (numbers, graphs)
\end{itemize}

% graph generation

% maybe: alternative graph failure because of noise and large variation

% statistics and their accuracy (run stats on the same dataset)

% ML models and their scores

% user satisfaction

\subsection{Evaluation}
\label{sec:evaluation}

%Tutkimustuloksien merkityst\"a on aina syyt\"a arvioida ja tarkastella
%kriittisesti.  Joskus tarkastelu voi olla t\"ass\"a osassa, mutta se
%voidaan my\"os j\"att\"a\"a viimeiseen osaan, jolloin viimeisen osan nimeksi
%tulee >>Tarkastelu>>. Tutkimustulosten merkityst\"a voi arvioida my\"os
%>>Johtop\"a\"at\"okset>>-otsikon alla viimeisess\"a osassa. 

%T\"ass\"a osassa on syyt\"a my\"os arvioida tutkimustulosten luotettavuutta.
%Jos tutkimustulosten merkityst\"a arvioidaan >>Tarkastelu>>-osassa,
%voi luotettavuuden arviointi olla my\"os siell\"a. 

% talk about success with graph generation
%  maybe talk about alt graph?
% finding bugs in system or faults based on graphs
% good feedback from users

% talk about negatives with machine learning models
% reason why they would turn out so bad

% talk about how the system went immediately into production
% talk about the success of notifications built on top of the system

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Conclusions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Conclusions and Future Work} 
\label{sec:conclusions}

\begin{itemize}
\item[--]Contributions (positives)
\item[--]Limitations (negatives)
\item[--]Future work
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
%% The \phantomsection command is necessary for hyperref to jump to the 
%% correct page, in other words it puts a hyper marker on the page.

\phantomsection
\addcontentsline{toc}{section}{References}

\printbibliography

%% Appendices
%\clearpage
%\thesisappendix

\end{document}
